# The Market for the African Software Developer

## Background
The title of this work is something of a misnomer as the reader will discover in the developing narrative. It is obviously deliberate and is probably a failed attempt at trolling, but it encapsulates a direct response to all those agents and principals who continue to evaluate the nature of the job and talent market of `African` software developers based on a premise whose Boolean value is not dissimilar from the main theme of [The Flat Earth Society](https://www.tfes.org), an organization that continues to  promote a fallacy across the world against centuries of scientific and empirical evidence. 

Here are the _fascinating_ ideas these entities have propagated across digital media:

- [African Slavery](https://nairametrics.com/artificial-intelligence-andela-and-africas-looming-technology-slavery-3-2/) risks being repackaged and reborn as global software and technology continue to advance at a breakneck pace, leaving the question of the economic development of Africa blowing in the wind.
- [A business whose mission is to train and export African software developers to the western hemisphere](https://nairametrics.com/artificial-intelligence-andela-and-africas-looming-technology-slavery-3-2/) effectively lowers the software development cost of its client companies, enabling them to remain globally competitive against local businesses and markets. (Troll: Do not assume `western hemisphere` here to be a proxy for any entity that can afford to pay standard, international prices for high quality software work, regardless of its GPS co-ordinates)
- A corollary of point 2 above is that this [export business activity](https://nairametrics.com/artificial-intelligence-andela-and-africas-looming-technology-slavery-3-2/) is not local market friendly, essentially harms local businesses' competitiveness and considerably weakens their ability to access cheap local talent, crowding them out of the market for high quality software work. An interesting phrase has been coined to describe and possibly provide a measure of this market dynamic - [_Ecosystem Impact Assessment (EIA)_](https://nairametrics.com/much-ado-nothing-andela-uproar/).
- What follows is that strategies adopted by a company that specializes in the described business area to meet its obligations and commitments to foreign clients, strategies such as hiring out elite (not in-house trained) developers for export, [creates imbalances within the identified ecosystem](https://nairametrics.com/much-ado-nothing-andela-uproar/), yielding a negative EIA value and, in the larger society, a significant nuisance value.
- There is a linear relationship between the narrowness of the hiring funnel of a company and the economic value it adds/subtracts to/from its country of operations. If a company - _in this case a software company_ - has depressingly low hiring rates - below 1% - this [does not add economic value](https://nairametrics.com/artificial-intelligence-andela-and-africas-looming-technology-slavery-3-2/) to its country of operations; therefore there exists a causative arrow from `high hiring rates` to `accretive domestic economic value`.

We hesitate to include private perspectives from closed messaging forums as they are not open to public scrutiny and the full context of comments and rebuttals may never be appreciated. 

  ### Readership
 This work, apart from being a response to certain simmering ideas, is directed at an audience interested in understanding issues beyond mere instances and anecdotes. The _no time_ skimmer is referred to the many resources available online specially packaged to meet their understandably unique reading requirements. 

 In order to work with the right context for responding to the concerns raised, certain elementary concepts governing the subject matter will be introduced and explained. In places where practice diverges from theory, these shall be noted accordingly. We have put in a lot of effort to keep things _as simple as possible but not simpler_ (useful mathematical analysis, for instance, has been skipped). Developing the main ideas from first principles and staying as close to primary sources as possible, let us begin.

## Distributed Networks and Scale
A cursory understanding of distributed systems and their properties is an important mental model for making sense of the issues at hand.

The geometry of the network (more correctly, _topology_) of the Internet is [distributed](https://www.youtube.com/watch?v=Dxcc6ycZ73M). Natural approximations of distributed systems can be found in many biological systems like the `brainless slime mold` and `schools of fish` which all evidently predate the Internet. The fundamental computing unit of a distributed network is a node. The figure below, image c shows what a distributed network looks like, relative to other forms of network topology.  For simplicity, we shall go ahead and conflate human operators and intelligent (and dumb) devices as nodes. What does this arrangement of transacting participants afford its users? 

<p>
<img src="resx\centralized-decentralized-distributed.jpeg">
<p>

- there is `no single point of failure`; this design property may seem inappropriately named. It does not imply that there is no single point of failure. It means that no single point of failure on a node or cluster of nodes can become an existential threat to the entire network. If one of Netflix's many web services falter, this event does not stop the interested reader from looking up extra materials on distributed systems on Google. Netflix services may be down but Google will respond to your query. Netflix and Google have millions of nodes deployed on the Internet. 
- the movement of data across the Internet is not `centrally planned` - there is no centralized arbiter dictating how data should be transmitted once it has left its originating node; the best pathway from origin to destination is algorithmically computed at the instant the system is required to move data from node to node based on standardized protocols.
- save for communist China and a couple others on the fringe of censorship (which can still be subverted), there are `no barriers` to the transmission and reception of data on the Internet. The Internet `flattens geographies`, giving little meaning to national boundaries and regional lines.
- advances in the supporting infrastructure of distributed networks from `cloud computing` to highly flexible degrees of `automation` have made it possible to `scale almost any idea` to billions of people on the planet. 

One of the direct implications of these assertions is that any arbitrage that would traditionally exist among pricing structures for various services in brick-and-mortar settings due to little or no information will now exist in an online business area only if a `node` (read here, a participating entity on the network) is unaware of what obtains on the distributed network for such services or, for some other reason, has chosen or been configured to be willfully ignorant - a dumb endpoint.

A simple analogy based on [human] nodes. Two hypothetical software developers, Alice and Bob, with the same skill sets basically have very similar potentials but the realization of these potentials is a function of how much network information they are able to extract and create value from and how consistently they are able to do this over time spent on the network. Alice, aware of the huge possibilities may position herself properly, advertise her portfolio of services on a personal website and social media, consistently deliver high quality on remote jobs, actively contribute to the value of the network through [various means](https://github.com) and generally increase (and improve) her visibility on the network. Bob, on the other hand, while possessing similar intrinsic value may never codify these abilities on the network to any significant degree of consistency or contribute these abilities to the overall network value - but he continues to remain locally (geographically) relevant. One has leveraged the `compounding properties` and configurations of the network to increase her value, while the other has unconsciously decided to restrict his sphere of relevance to his geographical location regardless of the characteristics of the network of which he is a member. While located in the same geographical region and possessing very similar skill sets, two software developers, `by their selfish choices`, have sharply diverged in their actual values as perceived by the network who, in this case, is the observer.

### From Nodes to Clusters
Our example of locally similar nodes attracting widely different premiums on the network can be extrapolated to clusters of nodes and related resources on the same network. An emergent property of distributed networks is that they tend to self-organize - motivated, high-value nodes may aggregate and increase their perceived value to the network so that _the whole [cluster] becomes greater than the sum of its parts_. `Power-law effects` introduce systemic inequalities as a natural result of this aggregation of high-value resources. Italian sociologist and philosopher `Vilfredo Pareto` laid the first foundations of power-law effects from his observations of income distributions and ultimately originated the generalized 80/20 rule which is now observable in fields and systems as varied as physics and financial markets. 

Power-law effects, for the case in point, mean that a cluster of nodes on the network contributes a disproportionate amount of value and resources to the network relative to its size. It also implies that this cluster extracts an outsized value from the network. It is important to state here that power laws are [scale invariant](https://en.wikipedia.org/wiki/Scale_invariance) - it is possible to observe their properties and effects at any scale given constant sample sizes. Value extracted by the cluster can range from pecuniary rewards to intangibles like goodwill and brand recognition. The morality and fairness of the equation - contribution versus gains - of this interaction between a cluster and its enveloping network is not the thrust of this work. It would also help to keep in mind that the general progress and survival of distributed networks has depended on more than a `consensus of the majority`. Adaptability and flexibility get higher marks. The high-value nodes and resources available to clusters also strengthen their reach and impact. This planetary scale characteristic engenders a self-reinforcing cycle that adds more value to the already very valuable cluster, sparking an unintended consequence of inequalities in the network.

We have now successfully described the technical operating environment of the _treacherous_ company described in the opening section (which is, in itself, obviously a type of cluster). Therefore we are clear on one axis of context. It should immediately evident that the incentive structures of distributed networks lean towards combining resources to deliver value to the network. It should also be clear that arbitrary combinations do not directly lead to the delivery of value. It is possible to have value destroying combinations but these obviously cannot withstand the `creative-destructive` forces that launch daily offensives on existing structures. Not for the long haul. Therefore the quality of the combinations surely matter. We shall now try to provide answers to the economic and social questions on why these combinations develop in the first place, and investigate how the nature and culture of distributed networks influence these answers.


## Open Source Philosophy and the Theory of the Firm
In November 1937, British economist Ronald Coase published his seminal treatise, [The Nature of the Firm](https://msuweb.montclair.edu/~lebelp/CoaseNatFirmEc1937.pdf), and sparked the discussion on why firms emerge, explaining that they are clusters of resources and agents where interactions are based on hierarchical, command systems rather than market-based channels. In relation to this theory, Coase also floated the term, _transaction costs_ - the broad costs of interfacing with and conducting business activities using the market system. According to his propositions, firms emerge when the gains of conducting some business activity is less in markets, _net of transaction costs_, relative to undertaking the same business activity within the organization, while they fade when the opposite is true - in-house costs are higher relative to the cost of interfacing and transacting through markets channels. Transaction costs are particularly important as the results of analysis here lead to tradeoffs which guide the decisions of firms on interfacing with markets. Various forms of this theory have largely explained the motivations for the emergence of companies across various industries especially as the prevailing production cost models of the last century were simply better looking derivatives of the industrial age. 

Open source technologies represent a direct challenge to this historical model of conducting business production activities. Open source, here, describes the publicly accessible nature of the development of some product (we restrict our scope of products to software for obvious reasons), leaving it open to modification and redistribution by its users. In this sense and to this extent, open source technologies are free. The open source movement, relying on the earlier described properties of distributed networks, provided a sustainable solution to the age-old problem of information asymmetry in firms (due to managerial hierarchy and command centralization) and markets ([due to the Agency Problem](https://en.wikipedia.org/wiki/Principal%E2%80%93agent_problem)), and shifted focus to the problem of the sourcing and optimal allocation of human ingenuity. The spread of the open source movement flies in the face of Coase's theory on the following grounds:

- the definition of transaction costs for interfacing firms with markets can no longer be generalized across industries as open source philosophies effectively break down this requirement for an interface - any entity who has need for an open source software product can go ahead and _fork_ it, modify the codebase and redistribute as required. At the simplest level, the end user does not need to retain the services of a law firm in order to fork a project neither does she need to remit some fraction of profits from a modified version of the product to the maintainer of the original codebase. This does not mean that there are no costs under this new paradigm; they exist in open source not in the traditional forms of property and contractual rights but appear in the form of organizational costs such as the cost of integrating teams, filtering out incompatible contributions from the forking public and the dynamics of continuous collaboration to produce some useful product and provide accompanying support. 

- in traditional markets, price signaling remains the general mechanism by which different agents and resources announce the cost of interfacing with them to achieve some business goal. This goes to the core of motivation and how agents in an economic system are incentivized to act in a particular (classically selfish) manner that delivers value to the whole network. Open source production systems do not follow this established signaling mechanism, not as directly. 


